# Getting Started with Big Data and Hadoop 2.x

> Big Data refers to the data which is **large, fast and complex type** of structured, semi-structured and unstructured data generated from a variety of different sources, which becomes **difficult to store and process** using a **traditional processing systems (RDBMS)**

**Challenges**

The two main challenges of Big Data

1. Storage : Distributed Storage System
2. Processing : MPP (Massive Parallel Processing System)

## Distributed System

> A Distributed System is a collection of computer systems that are physically separated but are linked together using the network

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXfcrjvq2htDIK7uIgh8c13lVhdHB09qLZ9AwQGo40NwalNo6hQKGlgOPaABgRXfBrpUO32b5NMyJxBpSpyu3hvULKjk5QymGs1soR336AtrtcPJgl_Pp4Px-WyB5dZBO-bXp_UpTL2wHJOCVTZuudKLWUA?key=ZDQI9yPkLwmZ3ZH_j9fetA)

## Hadoop 2.x (Distributed Storage and Processing System)

> Apache Hadoop is a software framework that allows us to **store and process large datasets** in parallel and distributed fashion

## Components of Hadoop

Hadoop consists of 3 main components

1. Storage Layer				:	**HDFS** (Hadoop Distributed File System)
2. Resource Management Layer       :         **YARN** (Yet Another Resource Negotiator)
3. Data Processing Layer                  :          **MapReduce**

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXen9AYoPfhvqr4alJaGs39OfKF53tbnppzxM9D751QYQJwEXku5dnvcRs8dMewJSTGr0guXQ6_q-O4xftC8yja653N9fbprZDVIB2UWwdnFHn9l4VT8AV66I1rZZtsFEhZG_aJwoCSYyrWE0lq2j_4XCcms?key=KG4XycolQz2vWFq2bNIfEQ)

**Note**

* <mark>Hadoop MapReduce is deprecated, alternate to Data Processing we use Apache Spark</mark>



Google released 2 white paper to explain Distributed System works

1. Map Reduce
2. GFS

Yahoo

1. Hadoop Map Reduce
2. HDFS

## Hadoop Daemon Services

5 Daemon services

1. NameNode
2. SecondaryNameNode
3. DataNode
4. ResourceManager
5. NodeManager

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXdo_k4ukwZzdHttPmE2t_keVoUeeC93iuGl_kI-sw5EcXbOy1oQ1u4E8GqPtmOG8GdoRL7ZQYlxMbprsHP3cA9L6BRp2rgiAEnxkP0mwb522Mq4DDWRiOuMhE8rRwYGNSACygRgrCKQvi6AZAeUz-pw9Mpa?key=KG4XycolQz2vWFq2bNIfEQ)

## Master and Slave Architecture

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXdYwEkLx-Tbkjf349yLl5yg4iPzFqNJHDUhpNDBT2Kvxm5E2OrjODcJqmzzfFtXyrj6iodWTbTm7MwAedOVvJwPhJBt2DiaAR5RJbyQ_D87HI-Q-w98O3ub77RjEPG3R7AyxXhFYmd7B_N9PS8ZjEh3IOke?key=KG4XycolQz2vWFq2bNIfEQ)

## HDFS and Architecture

> HDFS is a **Distributed and Scalable File System** designed for storing very large files.

**Distributed**

* In HDFS, files are stored across multiple machines, Instead of storing a file on a single computer, HDFS splits the file into smaller pieces, called blocks and distributes these blocks across different nodes in  a cluster.

  ![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXcupwNcbC5oa6XhOzsQiOl9hBaKCD-zCl18XLBSPJjy8cYggWxAj2B-AZDk2on8CJJ21milwQaU5Exu_5vzjjIvQIWrIJL6ysJcdGx0Zne5bcYXXek9uopaJ0-EWMdhpiBNKhOrX7-vAFAywn4NA-RovRUH?key=KG4XycolQz2vWFq2bNIfEQ)

* 500 MB file will be broken into blocks of size 128 MB (configurable)

* 500 MB 

  * Block 01 : 128 MB
  * Block 02 : 128 MB
  * Block 03 : 128 MB
  * Block 04 : 116 MB

**Scalable**

* It can grow in size

## HDFS Blocks and Replication



![](C:\MyTrainings\data-engineering-on-aws-infyni-02072024\Data Engineering with AWS Databricks and PySpark\imgs\01. Getting Started with Big Data and Hadoop 2.x\HDFS Architecture-Page-1.drawio.png)

```
cp web.log /tmp


hdfs dfs -put web.log /
hdfs dfs -setrep 1 /SMS
```

## Hadoop Ecosystem

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXcgnw9A0mEzbvlljTTZ8PAoeFnp3S-PT64eCJEfXtA-xLMJmVQdT9hMLhX4ZeFUeZ0skGBJXtmN0YlJpCgzmnFJlpFF1GNUWJFjFp5i_9ZCd0b8xM6p88AH4OE1_DdSj0-xQm2lzPiyi19yAewjv1FMAI5Y?key=KG4XycolQz2vWFq2bNIfEQ)

