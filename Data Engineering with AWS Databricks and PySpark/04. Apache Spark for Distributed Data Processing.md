# Apache Spark for Distributed Data Processing

> Apache Spark is an **in-memory** <u>cluster computing framework (Distributed Processing System)</u> designed to handle a wide range of big data workloads.

**Applications of Apache Spark**

Spark is designed to cover a wide range of workloads

* Data Integration and ETL
* High Performance Batch Computation
* Machine Learning Analytics
* Real-time stream processing
* Graph Computation

**Important Points**

* Apache Spark is natively written using Scala Programming

## What is PySpark

> PySpark is the **Python API for Apache Spark.**

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXcmW1c5W4L_ZuZKFd1EULvvSa0SFgzyXYZIkR-K6LgKmtSKoRWZ9gV8Dl0bmTzQWSK3hjJEHOENx8Er4NTbMJKmcO3gvgsoZeWwZKH6WLvAKat3hIXkks9CWSyMcuTM9InvHKXpL8vgjr33-bxht8g7V2c?key=uvmlVet7-pBAx-jz0PuzLA)

* The PySpark communicates with Spark using **Py4J**

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXc7gCkikotIUwYae-T4yk1SvLHio7fEcNB7YGTIj06q_RdD69HpI5vndqFKJ4_swRULrQJjGLj8qcBP9Qf7PHWXaB4Vk8LdtkQAOnIexHGcke9_JrCpznbVG1_3RB5Hsbg8eHICmWMwYXqw2XmkakRX4lg?key=uvmlVet7-pBAx-jz0PuzLA)

## Spark Ecosystem

![](C:\MyTrainings\data-engineering-on-aws-infyni-02072024\Data Engineering with AWS Databricks and PySpark\imgs\02. Apache Spark for Distributed Data Processing\Spark Ecosystem21-Page-3.drawio.png)

## Spark Interactive Shell

> SIS refers to a command line interface provide by Apache Spark , using which we can type and execute spark programs

Two main interactive shells

1. Spark Shell (Scala)
   - $> spark-shell
2. PySpark Shell
   - $> pyspark

## RDD's (Deprecated)

> Resilient Distributed Dataset are the building blocks of any spark application.

* Spark is built around RDD which is a fundamental data structure of spark used for parallel processing

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXd7VZz_C4R8emI0-bBFPm6MFtvdbk29b__8jaJI46oPxl8Z24xBF5NW1uPaIl0BTBthomqe0Zy6URYpQI1Jjmb_qULWBWDXjSvG6FtH7xtt7TzXie9fGSa5m9L7SbQB_Qr712_xRc8VIhNw0RXOt0QDpSXy?key=uvmlVet7-pBAx-jz0PuzLA)

## Partitions

> RDD is  collection of objects that is partitioned and distributed across nodes in a cluster.
>
> A partition in spark is a logical division of data stored on a node in the cluster

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXcRxYKdQWVudE8dbluW8wDPEpcgVfzH5xX4FGVl1s-ww197QhKMZTToEZ9LNauq63--v7gEJuE1QrdA10wKWovviuOVq3dxqGfCoU5uQUDL4ASMgTB0k-PeH2l2IRLXhTYSqNxjbkTXsuav7LYskFxGhx4y?key=uvmlVet7-pBAx-jz0PuzLA)`

## RDD Creation

There are two main ways to create an RDD

1. Create an RDD from Collection 

   ```
   L = [1,2,3,4,5,6,7]
   rdd = sc.parallelize(L)
   print(type(rdd))
   ```

2. Create an RDD from external source

   ```
   file_name = "c:/users_001.csv"
   rdd = sc.textFile(file_name)
   print(type(rdd))
   ```

   

**Important point**

* All the methods to create an RDD is present inside SparkContext (sc)

## RDD Operations

![](C:\MyTrainings\data-engineering-on-aws-infyni-02072024\Data Engineering with AWS Databricks and PySpark\imgs\02. Apache Spark for Distributed Data Processing\unnamed.png)

## Self Study

* How Hadoop MapReduce work and its limitation